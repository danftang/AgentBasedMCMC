\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{amssymb,amsmath,amsthm}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{apacite}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}
\DeclareMathOperator\supp{supp}

\title{Sampling from the Bayesian poster of an agent-based model given partial observations}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
  Daniel Tang\\
  Leeds Institute for Data Analytics\thanks{This project has received funding from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No. 757455)}\\
  University of Leeds\\
  Leeds, UK\\
  \texttt{D.Tang@leeds.ac.uk} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
}


\begin{document}
\maketitle

\begin{abstract}
abstract
\end{abstract}

% keywords can be removed
\keywords{Data assimilation, Bayesian inference, Agent based model, Integer linear programming, predator prey model}

\section{Introduction}
%##########################################


Suppose we have a timestepping ABM where agents have a finite number of possible internal states and a finite number of ways of acting on their world. Given this, we can define an ABM as:
\begin{itemize}
\item A domain of agent actions $\mathcal{A} =\{ a_0 ... a_n \}$

\item A domain of agent states $\mathcal{S} = \{\sigma_0 ... \sigma_m\}$

\item An agent timestep function, $a = \pi(\psi,\Psi)$, which is a computer program whose input is the agent's state $\psi \in \mathcal{S}$ and a (sparse) vector, $\Psi$, whose elements $\psi_i$ are the number of agents in state $\sigma_i$ in the model, and whose output is a chosen action $a \in \mathcal{A}$. The program may make calls to a random number generator, so the function is stochastic and we write $P(\pi(\psi,\Psi)=a)$ to be the probability that the program will return action $a$, given inputs $\psi$ and $\Psi$.

\item An agent transition tensor $F_\phi^{\psi a}$ whose elements give the number of agents in state $\phi$ that result from an agent in state $\psi$ performing act $a$ (including the acting agent).
\end{itemize}

Let a model timestep consist of a matrix $E$ whose elements $e_{\psi a}$ are the number of agents in state $\psi$ that perform act $a$ in this timestep. Similarly, let a model trajectory be a tensor $T^t_{\psi a}$ whose elements are the number of agents in state $\psi$ that perform act $a$ in timestep $t$.

A trajectory must satisfy a number of constraints in order to be a possible trajectory of an ABM. Since the elements of a trajectory are counts, they must all be non-negative integers. So we have the \textit{non-negative integer constraint} that
\begin{equation}
\forall t,\psi, a: T^t_{\psi a} \in \mathbb{Z}_{\ge 0}
\label{nonNegativeInt}
\end{equation}

A trajectory, $T^t_{\psi a}$, must also be \textit{continuous} which means that each agent that results from the actions of an agent in timestep $t$ must appear in the actions of timestep $t+1$. This can be expressed in terms of the \textit{continuity constraints}:
\begin{equation}
\forall t \in 1 ... n:\forall \phi: \sum_{\psi, a} F_\phi^{\psi a}T^{t-1}_{\psi a} - \sum_a T^t_{\phi a} = 0
\label{continuous}
\end{equation}

If a trajectory satisfies constraints \ref{nonNegativeInt} and \ref{continuous} we say that it is \textit{valid}.

Suppose we have a prior belief, $P_0(\Psi)$, over the state of the ABM at time $t=0$. The prior probability of a trajectory is then given by
\[
P(T^t_{\psi a}) =
\begin{cases}
P_0(\sum_aT^0_{\psi a}) \prod_{t, \psi, a} P(\pi(\psi,\sum_bT^{t}_{\phi b})=a)^{T^{t}_{\psi a}} & \text{if } T^t_{\psi a} \text{ is valid} \\
0 & \text{otherwise}\\
\end{cases}
\]

Suppose we also have a set of noisy, aggregate observations, $\Omega$ and an easily computed likelihood function $P(\Omega|T^t_{\psi a})$. Our aim is to generate a number of samples from the posterior distribution
\[
P(T^t_{\psi a}|\Omega) \propto P\left(\Omega \middle| T^{t}_{\psi a}\right)P(T^t_{\psi a})
\]

We assume that each observation is independent of the others given the trajectory, so that
\[
P(\Omega|T^t_{\psi a}) = \prod_{(\omega,v) \in \Omega} P(\omega=v|T^t_{\psi a})
\]

Also, for notational convenience, we express our prior belief in terms of a set of observations, $\Omega_0$, at $t=0$, so that $P_0(\sum_a T^0_{\psi a}) \propto \prod_{(\omega,v) \in \Omega_0} P(\omega=v|T^t_{\psi a})$ (the same can be done for any boundary conditions that aren't at $t=0$). The posterior can now be written as
\begin{equation}
P(T^t_{\psi a}|\Omega) \propto 
\begin{cases}
\prod_{(\omega,v) \in \Omega} P\left(\omega=v \middle| T^{t}_{\psi a}\right) \prod_{t, \psi, a}P(\pi(\psi,\sum_bT^{t}_{\phi b})=a)^{T^{t}_{\psi a}} & \text{if } T^t_{\psi a} \text{ is valid} \\
0 & \text{otherwise}\\
\end{cases}
\label{posterior}
\end{equation}

This posterior has zero probability for the vast majority of trajectories because most trajectories are invalid, contain an impossible action or are refuted by the observations. This makes sampling from this distribution vary difficult in practice. For example, even though we can generate samples from the prior by simply performing a forward execution of the model, it is often very unlikely that this sample will fit the observations, so techniques such as rejection sampling are not practical.

Our strategy in this paper will be to use Markov Chain Monte-Carlo sampling. However, this requires a proposal function which generates a new sample from the current one. This is often done by randomly perturbing the current sample. However, if we randomly perturb an ABM trajectory it's very unlikely that we'll end up with a trajectory that is valid, contains only possible actions and satisfies the observations. In practice, then, a random perturbation would almost always be rejected and we'd probably end up stuck on the initial sample until we grew old.

\section{Approximating the support of the posterior}
%##########################################


In order to solve this problem, we'll make the states of our Markov chain an approximation of the support of the posterior, $P(T^t_{\psi a}|\Omega)$, (i.e. the set of trajectories that have non-zero probability) and restrict our proposal function to perturbations that are in this approximation.

If we let
\[
P^t_{\pi \xi c}(T^s_{\psi a}) = P(\pi(\xi,\sum_bT^{t}_{\phi b})=c)^{T^{t}_{\psi a}}
\]
Then from equation \ref{posterior}
\begin{equation}
\supp (P( \,.\, |\Omega)) = \bigcap_{(\omega,v) \in \Omega}  \supp(P(\omega = v \mid . )) \cap \bigcap_{t, \xi, c} \supp(P^t_{\pi \xi c})
\label{support}
\end{equation}
i.e. in order for the posterior to be non-zero, all the observation likelihoods must be non-zero and the probability of each agent's action at every timestep must be non-zero.

In order to approximate this support, we first express each likelihood function $P(\omega=v|T^t_{\psi a})$ as a computer program $\omega(T^t_{\psi a})$ whose input is a trajectory and whose output is the value of the observation. This program may make calls to a random number generator, so the function is stochastic and we write $P(\omega(T^t_{\psi a}) = v)$ to denote the probability that $\omega(T^t_{\psi a})$ returns a value $v$. So equation \ref{support} now consists of the intersection of terms of the form $\supp(P(\rho(.) = y))$ where $\rho$ is a computer program whose input is a trajectory and y is a given output. So, we need a way to find, or approximate, the set of inputs of a computer program that would produce a given output, and to take the intersection of these sets. Our strategy here is to calculate a set of linear inequalities, $\mathcal{P}$, on trajectories that are satisfied for all $x$ such that $\rho(x)$ might return $y$. Note that the converse is not necessarily true; if $x$ satisfies $\mathcal{P}$ then it might not be possible for $\rho(x)$ to return $y$. So $\mathcal{P}$ can be thought of as an approximation of $\supp(P(\rho(.)=y))$ in that it may include some extra members. However, for our purposes, a certain amount of approximation is tolerable and won't result in any approximation in the Markov Chain. If we happen to choose a proposal from $\mathcal{P}$ that isn't in $\supp(P(\rho(.)=y))$ then it will just be rejected. This is fine as long as we generate acceptable proposals at a reasonable rate.

A computer program can be transformed into a set of linear inequalities using a technique known as \textit{abstract interpretation}\cite{cousot1977abstract}. Tools to do this already exist (e.g. PAGAI\cite{henry2012pagai}) and are used widely in applications such as the verification of safety critical systems\cite{blanchet2003static}. For our application we choose the abstract domain of convex polyhedra\cite{cousot1978automatic}\cite{becchi2018efficient}. Calls to a random number generator are dealt with, without loss of generality, by providing a single function \texttt{rand()} that returns a floating point number between 0 and 1 with uniform probability. In the abstract domain, this is equivalent to the generation of a new variable, $r$, that satisfies $0 \le r < 1$.

Once we have the linear approximations of the support of each program, the intersection of these supports is just the set of trajectories that satisfy all of the inequalities. So, if we let $\mathcal{P}(\rho(.)=v)$ be the set of inequalities which approximate $\supp(P(\rho(.)=v))$ then we have the \textit{support inequalities}
\begin{equation}
\supp(P(P(.|\Omega))) \subset \bigcap_{(\omega,v) \in \Omega}  \mathcal{P}(\omega(.) = v) \cap \bigcap_{t, \xi, c} \mathcal{P}(\pi'(\xi,t,.)=c)
\label{linearSupport}
\end{equation}
where
\[
\pi'(\xi, s, T^t_{\psi a}) = \pi(\xi,\sum_bT^{s}_{\phi b})
\]
In order to ensure the trajectory is valid, we also intersect with the constraints in \ref{continuous}, which are also linear. We can express the non-negative part of equation \ref{nonNegativeInt} as the linear constraints
\begin{equation}
\forall t, \psi, a: T^t_{\psi a} \ge 0.
\label{nonNegative}
\end{equation}

The integer part we will consider in the next section.

\section{From convex polyhedron to Markov process}
%#####################################################

Once we've generated a set of inequalities that contain the support of the posterior, we next need to form a Markov process whose states emit solutions to the inequalities and whose stationary distribution is the posterior $P(T \mid \Omega)$. I order to perform MCMC we'd like to define the Markov process in terms of a set of states, $\mathbb{S}$, and a stochastic proposal function $f:\mathbb{S} \to \mathbb{S}$ such that $P(f(s_n) = s_{n+1})$ defines the conditional transition probability $P(s_{n+1} \mid s_n)$. The proposal function should have the following properties:
\begin{itemize}
\item There should be a set of transitions with non-zero probability between any two states. This ensures proper mixing as time tends to infinity.

\item The probability of emitting a trajectory $T$ should be our target posterior probability $P(T \mid \Omega)$, and all emitted trajectories should be solutions of our set of linear equations.

\item For any transition from state $s_a \to s_b$ the probability of transitioning from $s_b \to s_a$ should be non-zero. This allows us to use the Metropolis Hastings algorithm to attain detailed balance.

\item The proposal function should be computationally efficient. 
\end{itemize}

\subsection{A proposal function for a Fermionic ABM}
%#############################################

We define a Fermionic ABM to be one where no two agents can share the same state at the same time. i.e. in addition to the validity constraints of equation \ref{nonNegativeInt} and \ref{continuous} the trajectory of a Fermionic ABM also satisfies the \textit{Fermionic constraint}
\begin{equation}
\forall t,\psi: \sum_a T^t_{\psi a} \le 1
\label{fermionic}
\end{equation}
in which case we call it a \textit{Fermionic trajectory}.

\begin{theorem}
All valid Fermionic trajectories are on the vertices of the convex polyhedron described by equations \ref{continuous}, \ref{nonNegative}, \ref{fermionic} and \ref{linearSupport}.
\end{theorem}
\begin{proof}
This can be seen immediately since equations \ref{nonNegative} and \ref{fermionic} describe a unit hupercube, so all integer solutions are on vertices.
\end{proof}

\begin{theorem}
For any pair of valid trajectories, $A$ and $B$, there exists a path along edges of the polyhedron from $A$ to $B$ that only traverses through integer solutions.
\end{theorem}
\begin{proof}

[Take the difference A-B and decompose into integer null vectors (loops)]

\end{proof}

Since all Fermionic trajectories are on vertices, the obvious way forward would be to define a Markov process whose states, $\mathbb{S}$, are the vertices of the polyhedron and whose transitions are its edges. So, the proposal function would choose from the neighbouring vertices of the current state. There may exist vertices that don't fall on the grid of integer solutions, so are not associated with valid trajectories. However, extra states can be added to a Markov process without affecting the emitted samples as long as the extra states do not emit samples (i.e. the process can pass through these states, but no sample is generated when doing so). However, we don't want to spend too much computational effort moving between non-emitting states, so we should ensure that the probability of being in a non-emitting state isn't much greater than that of being in an emitting state. In order to encourage the Markov process away from these fractional states their probability is multiplied by a ``fractional penalty'' $e^{-kn}$ where $n$ is the number of non-integer values in the solution and $k$ is a constant parameter.

This approach is computationally convenient as navigation along edges of a polyhedron can be achieved efficiently by using the pivoting operation of the Simplex algorithm\cite{dantzig1955generalized}\cite{thie2011introduction}. Given a system of $m$ linear equality constraints on $n$ variables (where any inequalities are turned into equalities by the addition of slack variables), let a \textit{pivot state} be a partition of the variables into $m$ \textit{basic variables} and $n-m$ \textit{non-basic variables} such that if we constrain all \textit{non-basic variables} to zero there remains a unique solution. Such solutions can be shown to be at a vertex of the polyhedron described by the constraints. A \textit{pivot} is a perturbation of a pivot state such that one of the basic variables is swapped with one of the non-basic variables so the set of basic variables loses one of its members and gains a new member from the set of non-basic variables, such that the perturbed state is also a valid pivot state. It can be shown that every edge of a convex polyhedron corresponds to a unique pivot, so moving along an edge can be achieved computationally by performing a pivot.

This use of pivoting is complicated somewhat when the vertices of the polyhedron are degenerate. A degenerate vertex is one that can be generated by more than one pivot state. This can occur when any of the basic variables are zero, since a pivot has the effect of making a basic variable non-basic, so its value will be constrained to zero; but if the basic-variable's value is zero to start with, constraining it to zero has no effect. If there are many zero-valued basic variables there may be a large number of pivots, and even sequences of pivots, that do not change the solution, and consequently many pivot states which have the same solution. This has the consequence that a degenerate vertex may have a very large number of neighbours, so choosing one at random becomes a non-trivial task since it would be computationally costly to simply enumerate them all and choose one\cite{gal1992new}\cite{yamada1994enumerating}.

An alternative strategy is to replace the Markov states representing degenerate vertices with multiple states. As long as they all emit the same trajectory and the sum of their probabilities is the probability of the vertex then the Markov process will still emit samples from the posterior.

Let an \textit{ordered tableau} be a simplex tableau where all degenerate rows (i.e. rows with value zero) are above all non-degenerate rows, and non-degenerate rows are ordered by the index of the variable that is pivoted-in on that row. We now associate a Markov state with each ordered tableau, assigning it a probability according to algorithm \ref{probAlgorithm}:

\begin{algorithm}
\caption{Algorithm to calculate probability of a degeneracy state}
\label{probAlgorithm}
\begin{algorithmic}
\State $A \leftarrow$ an ordered tableau
\State $P_T \leftarrow$ the probability of the vertex that is the solution to this tableau
\State $P_d \leftarrow 1.0$
\State $\sigma \leftarrow$ the number of degenerate rows in $A$
\For{$i = 0 \dots \sigma-1$}
	\State $Cols \leftarrow$ the set of columns in A that have at least one non-zero value in rows $i \le r < \sigma$
	\State $P_d \leftarrow P_d/|Cols|$
\EndFor
\State \Return $P_d \times P_T$
\end{algorithmic}
\end{algorithm}

The transitions between states are the valid pivots and the swapping of position of any two degenerate rows. The proposal function is shown in algorithm \ref{proposal}

\begin{algorithm}
\caption{Proposal function}
\label{proposal}
\begin{algorithmic}
\Function{Proposal}{A}
\State $A' \leftarrow A$ (the ordered tableau of the current state)
\State $\alpha, \beta \leftarrow$ constant parameters
\State $p_0 \leftarrow$ the set of degenerate pivots of A (i.e. the ones that do not change the solution)
\State $p_1 \leftarrow$ the set of non-degenerate pivots of A (i.e. the ones that change the solution)
\If{\Call{Bernoulli}{$\alpha$}}
  \State $r \leftarrow$ choose member of $p_1$ with uniform probability
  \State perform pivot $r$ on $A'$
\ElsIf{\Call{Bernoulli}{$\beta$}}
  \State $r \leftarrow$ choose member of $p_0$ with uniform probability
  \State perform pivot $r$ on $A'$
\Else
  \State choose two degenerate rows $a$ and $b$ with uniform probability
  \State swap rows $a$ and $b$ of $A'$ 
\EndIf
\State \Return $A'$
\EndFunction
\end{algorithmic}
\end{algorithm}

We need to show that this assignment of probability to ordered tableaus has the property that the sum of the probabilities associated with a vertex is the probability of that vertex.

\begin{theorem}
If $\mathbb{E}(T)$ is the set of all ordered tableaus with solution $T$, and $P(A)$ is the probability assigned to ordered tableau $A$ by algorithm \ref{probAlgorithm} and $P_T$ is the probability assigned to vertex $T$ then
\[
\sum_{A \in \mathbb{E}(T)} P(A) = P_T
\]
\end{theorem}
\begin{proof}
Consider the stochastic algorithm \ref{unityProof}. It is clear that every possible output is an ordered tableau with solution $T$ and that every ordered tableau with solution $T$ is a possible output of the algorithm. The probability that algorithm \ref{unityProof} outputs tableau $A$ is
\[
\frac{1}{\prod_{i=0}^{\sigma-1} C_i^\sigma(A)} 
\]
where $C_i^\sigma(A)$ is the number of columns that have at least one non-zero value in rows $i \le r < \sigma$ in $A$ since $C_i^\sigma(A)$ is invariant under pivots on rows $i \le r < \sigma$. But this is exactly the probability $P_d$ in algorithm \ref{probAlgorithm}.

It is also clear that, with probability 1, the algorithm returns an ordered tableau, so the sum of the probabilities of all possible outputs of algorithm \ref{unityProof} must also be 1. So
\[
\sum_{A \in \mathbb{E}(T)} P_d(A) = 1
\]
but $P(A) = P_d(A)P_T$ so 
\[
\sum_{A \in \mathbb{E}(T)} P(A) = \sum_{A \in \mathbb{E}(T)} P_d(A)P_T = P_T
\]
\end{proof}

\begin{algorithm}
\caption{Random choice of ordered tableau with solution T}
\label{unityProof}
\begin{algorithmic}
\State $A \leftarrow $ any ordered tableau with solution $T$
\For{$i = 0 \dots \sigma-1$}
  \State $Cols \leftarrow$ the set of columns in A that have at least one non-zero value in rows $i \le r < \sigma$
  \State $r \leftarrow$ random member of $Cols$ chosen with uniform probability
  \State pivot on column $r$ and lowest index row, $i \le r < \sigma$, of $A$
  \State swap the newly pivoted row and the $i^{th}$ row of $A$
\EndFor
\State \Return A
\end{algorithmic}
\end{algorithm}

Given this, it's clear that all the necessary properties of the proposal function are fulfilled.

%\bibliographystyle{unsrtnat}
%\bibliographystyle{apalike} 
\bibliographystyle{apacite}
\bibliography{references}

\end{document}
